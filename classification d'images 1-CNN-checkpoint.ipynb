{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description du problème:\n",
    "Dans le cadre de ce projet ,on sera amené a élaborer des modèles de classification d'images afin de permettre aux\n",
    "potentiels clients utilisant notre site de vente de vetements d'accéder facilement a toutes les classes de vetements dont\n",
    "on dispose .Pour cela on acommencé d'abord par un mod-le d'apprentissage profond qui sera détaillé dans ce qui suit.\n",
    "On a pas pu télécharger les données en utilisant pickle pour ce premier modèle ,on les a alors téléchargé depuis internet.Mais on a \n",
    " reussi a travailler avec le dataset fourni pour les 2 autres modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des différentes bibliothèques necessaires.\n",
    "on importera le package tensorflow ou on va compiler notre code ,ainsi que les différentes modules soit de visualistion de\n",
    "données ou de gestion de fichiers etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réglage des parametres de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IMG_ROWS = 28\n",
    "IMG_COLS = 28\n",
    "NUM_CLASSES = 10\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 100\n",
    "#Model\n",
    "NO_EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séparation des données en deux files,le pemier pour le training et le deuxième pour le testing.\n",
    "on avait pas au départ le directory voulue ,donc on a changé le directory sur le fichier ou on a déposé nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\hp\\Desktop\\tp classification')\n",
    "train_file = \"fashion_mnist_train.csv\"\n",
    "test_file  = \"fashion_mnist_test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation de chacune des dimensions de notre dataset,le Training Dataset et le testing Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion MNIST train -  rows: 60000  columns: 785\n",
      "Fashion MNIST test -  rows: 10000  columns: 785\n"
     ]
    }
   ],
   "source": [
    "print(\"Fashion MNIST train -  rows:\",train_data.shape[0],\" columns:\", train_data.shape[1])\n",
    "print(\"Fashion MNIST test -  rows:\",test_data.shape[0],\" columns:\", test_data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation du nombre d'images pour chaque classe ,pour le training dataset et pour le testing dataset.\n",
    "on remarquera que les classes sont équitablement distribués que ca soit pour le training dataset ou le testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prétraitement des données\n",
    "Nous allons d’abord effectuer un prétraitement des données pour préparer le modèle.\n",
    "\n",
    "Nous remodelons les colonnes de (784) à (28,28,1). Nous enregistrons également la fonctionnalité label (cible) en tant que vecteur distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def data_preprocessing(raw):\n",
    "    out_y = keras.utils.to_categorical(raw.label, NUM_CLASSES)\n",
    "    num_images = raw.shape[0]\n",
    "    x_as_array = raw.values[:,1:]\n",
    "    x_shaped_array = x_as_array.reshape(num_images, IMG_ROWS, IMG_COLS, 1)\n",
    "    out_x = x_shaped_array / 255\n",
    "    return out_x, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "X, y = data_preprocessing(train_data)\n",
    "X_test, y_test = data_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division des données\n",
    "on divisera les données de training rn deux parties,une partie de training et une autre pour la validation .le pourcentage\n",
    "de celle de la validation sera de l'ordre de 10% de celui du training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion MNIST train -  rows: 54000  columns: (28, 28, 1)\n",
      "Fashion MNIST valid -  rows: 6000  columns: (28, 28, 1)\n",
      "Fashion MNIST test -  rows: 10000  columns: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Fashion MNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:4])\n",
    "print(\"Fashion MNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:4])\n",
    "print(\"Fashion MNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description du modèle:\n",
    "Nous utiliserons le modèle séquentiel  qui est une pile linéaire de couches. le modèle sera initialisé,\n",
    "puis nous ajoutons a cahaque fois les couches suivantes:\n",
    "    \n",
    "    2 Conv2D ,la premiere avec un filtre de 32,un kernel size de(3,3) qui  réfère ici à la largeur x hauteur du masque de filtre,\n",
    "    puis une fonction d'activation de type relu=max(0,a)et le deuxiemme avec un filtre de 128,avec le meme kernel size\n",
    "    etla meme fonction d'activation relu puisq'il se trouve qu'elle est plus efficace que le sigmoide.\n",
    "    \n",
    "    2 Maxpooling qui renvoient le pixel dont la valeur maximale provient d'un ensemble de \n",
    "    pixels contenus dans un masque (noyau).avec un poolsize de (2,2)pool_size,représentant les facteurs permettant\n",
    "    de réduire la dimension  dans les deux sens.\n",
    "    \n",
    "     a flatten  qui prend ne copie du tableau en entrée, ajustée à une dimension\n",
    "\n",
    "    2 Dense implémente l'opération: output = activation (point (entrée, noyau) + biais) où activation est la fonction\n",
    "     d'activation élément par élément transmise en tant qu'argument d'activation, kernel est une matrice de pondérations\n",
    "     créée par la couche, et biais est un vecteur de biais créé par la couche,on l'utilise la première pour réduire\n",
    "    la dimension a 128,et la deuxième fois pour réduire la dimension a 10(nbre de classes).\n",
    "    \n",
    "     et finalement,nous compilons le modèle en spécifiant également les paramètres suivants:perte ,optimizeur et métrique.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "# Add convolution 2D\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exécuter le modèle\n",
    "Nous exécutons le modèle avec l'ensemble de formation. Nous utilisons également le jeu de validation \n",
    "(un sous-jeu du jeu d’entraînement original) pour la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "54000/54000 [==============================] - 46s 845us/sample - loss: 0.5250 - acc: 0.8099 - val_loss: 0.3623 - val_acc: 0.8773\n",
      "Epoch 2/30\n",
      "54000/54000 [==============================] - 44s 823us/sample - loss: 0.3371 - acc: 0.8793 - val_loss: 0.3145 - val_acc: 0.8858\n",
      "Epoch 3/30\n",
      "54000/54000 [==============================] - 45s 826us/sample - loss: 0.2912 - acc: 0.8926 - val_loss: 0.2994 - val_acc: 0.8893\n",
      "Epoch 4/30\n",
      "54000/54000 [==============================] - 45s 826us/sample - loss: 0.2585 - acc: 0.9053 - val_loss: 0.2777 - val_acc: 0.8962\n",
      "Epoch 5/30\n",
      "54000/54000 [==============================] - 45s 828us/sample - loss: 0.2358 - acc: 0.9135 - val_loss: 0.2571 - val_acc: 0.9095\n",
      "Epoch 6/30\n",
      "54000/54000 [==============================] - 45s 834us/sample - loss: 0.2123 - acc: 0.9208 - val_loss: 0.2505 - val_acc: 0.9110\n",
      "Epoch 7/30\n",
      "54000/54000 [==============================] - 45s 830us/sample - loss: 0.1927 - acc: 0.9289 - val_loss: 0.2667 - val_acc: 0.9045\n",
      "Epoch 8/30\n",
      "54000/54000 [==============================] - 45s 832us/sample - loss: 0.1742 - acc: 0.9348 - val_loss: 0.2453 - val_acc: 0.9113\n",
      "Epoch 9/30\n",
      "54000/54000 [==============================] - 45s 827us/sample - loss: 0.1575 - acc: 0.9417 - val_loss: 0.2666 - val_acc: 0.9142\n",
      "Epoch 10/30\n",
      "54000/54000 [==============================] - 45s 829us/sample - loss: 0.1407 - acc: 0.9471 - val_loss: 0.2822 - val_acc: 0.9083\n",
      "Epoch 11/30\n",
      "54000/54000 [==============================] - 45s 829us/sample - loss: 0.1260 - acc: 0.9529 - val_loss: 0.2775 - val_acc: 0.9137\n",
      "Epoch 12/30\n",
      "54000/54000 [==============================] - 45s 831us/sample - loss: 0.1141 - acc: 0.9573 - val_loss: 0.2638 - val_acc: 0.9142\n",
      "Epoch 13/30\n",
      "54000/54000 [==============================] - 45s 830us/sample - loss: 0.0994 - acc: 0.9621 - val_loss: 0.2767 - val_acc: 0.9167\n",
      "Epoch 14/30\n",
      "54000/54000 [==============================] - 45s 835us/sample - loss: 0.0906 - acc: 0.9660 - val_loss: 0.2946 - val_acc: 0.9163\n",
      "Epoch 15/30\n",
      "54000/54000 [==============================] - 45s 830us/sample - loss: 0.0794 - acc: 0.9706 - val_loss: 0.3253 - val_acc: 0.9158\n",
      "Epoch 16/30\n",
      "54000/54000 [==============================] - 45s 832us/sample - loss: 0.0720 - acc: 0.9730 - val_loss: 0.3534 - val_acc: 0.9162\n",
      "Epoch 17/30\n",
      "54000/54000 [==============================] - 45s 828us/sample - loss: 0.0639 - acc: 0.9762 - val_loss: 0.3394 - val_acc: 0.9132\n",
      "Epoch 18/30\n",
      "54000/54000 [==============================] - 45s 832us/sample - loss: 0.0553 - acc: 0.9792 - val_loss: 0.3944 - val_acc: 0.9117\n",
      "Epoch 19/30\n",
      "54000/54000 [==============================] - 45s 828us/sample - loss: 0.0547 - acc: 0.9800 - val_loss: 0.3822 - val_acc: 0.9073\n",
      "Epoch 20/30\n",
      "54000/54000 [==============================] - 45s 831us/sample - loss: 0.0466 - acc: 0.9830 - val_loss: 0.4103 - val_acc: 0.9143\n",
      "Epoch 21/30\n",
      "54000/54000 [==============================] - 45s 827us/sample - loss: 0.0430 - acc: 0.9842 - val_loss: 0.4073 - val_acc: 0.9215\n",
      "Epoch 22/30\n",
      "54000/54000 [==============================] - 45s 829us/sample - loss: 0.0376 - acc: 0.9859 - val_loss: 0.4544 - val_acc: 0.9147\n",
      "Epoch 23/30\n",
      "54000/54000 [==============================] - 45s 828us/sample - loss: 0.0404 - acc: 0.9849 - val_loss: 0.4364 - val_acc: 0.9092\n",
      "Epoch 24/30\n",
      "54000/54000 [==============================] - 45s 827us/sample - loss: 0.0394 - acc: 0.9858 - val_loss: 0.4633 - val_acc: 0.9138\n",
      "Epoch 25/30\n",
      "54000/54000 [==============================] - 48s 884us/sample - loss: 0.0272 - acc: 0.9903 - val_loss: 0.4800 - val_acc: 0.9155\n",
      "Epoch 26/30\n",
      "54000/54000 [==============================] - 45s 833us/sample - loss: 0.0270 - acc: 0.9899 - val_loss: 0.4915 - val_acc: 0.9128\n",
      "Epoch 27/30\n",
      "54000/54000 [==============================] - 45s 832us/sample - loss: 0.0319 - acc: 0.9881 - val_loss: 0.5320 - val_acc: 0.9118\n",
      "Epoch 28/30\n",
      "54000/54000 [==============================] - 45s 830us/sample - loss: 0.0278 - acc: 0.9896 - val_loss: 0.5141 - val_acc: 0.9157\n",
      "Epoch 29/30\n",
      "54000/54000 [==============================] - 45s 829us/sample - loss: 0.0287 - acc: 0.9900 - val_loss: 0.5325 - val_acc: 0.9118\n",
      "Epoch 30/30\n",
      "54000/54000 [==============================] - 45s 842us/sample - loss: 0.0216 - acc: 0.9925 - val_loss: 0.4984 - val_acc: 0.9117\n"
     ]
    }
   ],
   "source": [
    "train_model = model.fit(X_train, y_train,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=NO_EPOCHS,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactitude de prévision de test\n",
    "Nous calculons la perte de test et la précision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4962173754028976\n",
      "Test accuracy: 0.9121\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nouveau modèle\n",
    "Nous ajoutons plusieurs couches de drop out au modèle afin d'éviter le surapprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "# Add convolution 2D\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 input_shape=(IMG_ROWS, IMG_COLS, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "# Add dropouts to the model\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, \n",
    "                 kernel_size=(3, 3), \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Add dropouts to the model\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# Add dropouts to the model\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# Add dropouts to the model\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.7447 - acc: 0.7204 - val_loss: 0.4771 - val_acc: 0.8293\n",
      "Epoch 2/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.4679 - acc: 0.8277 - val_loss: 0.3755 - val_acc: 0.8610\n",
      "Epoch 3/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.4016 - acc: 0.8538 - val_loss: 0.3266 - val_acc: 0.8780\n",
      "Epoch 4/30\n",
      "54000/54000 [==============================] - 54s 1ms/sample - loss: 0.3668 - acc: 0.8665 - val_loss: 0.3188 - val_acc: 0.8860\n",
      "Epoch 5/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.3408 - acc: 0.8743 - val_loss: 0.2844 - val_acc: 0.8955\n",
      "Epoch 6/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.3217 - acc: 0.8826 - val_loss: 0.2851 - val_acc: 0.8977\n",
      "Epoch 7/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.3098 - acc: 0.8860 - val_loss: 0.2566 - val_acc: 0.9028\n",
      "Epoch 8/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2965 - acc: 0.8898 - val_loss: 0.2543 - val_acc: 0.9072\n",
      "Epoch 9/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2852 - acc: 0.8961 - val_loss: 0.2548 - val_acc: 0.9058\n",
      "Epoch 10/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2818 - acc: 0.8949 - val_loss: 0.2406 - val_acc: 0.9150\n",
      "Epoch 11/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2735 - acc: 0.8979 - val_loss: 0.2478 - val_acc: 0.9103\n",
      "Epoch 12/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2661 - acc: 0.9012 - val_loss: 0.2356 - val_acc: 0.9147\n",
      "Epoch 13/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2643 - acc: 0.9025 - val_loss: 0.2325 - val_acc: 0.9150\n",
      "Epoch 14/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2572 - acc: 0.9051 - val_loss: 0.2304 - val_acc: 0.9148\n",
      "Epoch 15/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2503 - acc: 0.9069 - val_loss: 0.2320 - val_acc: 0.9132\n",
      "Epoch 16/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2489 - acc: 0.9076 - val_loss: 0.2307 - val_acc: 0.9123\n",
      "Epoch 17/30\n",
      "54000/54000 [==============================] - 56s 1ms/sample - loss: 0.2441 - acc: 0.9090 - val_loss: 0.2330 - val_acc: 0.9178\n",
      "Epoch 18/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2429 - acc: 0.9096 - val_loss: 0.2275 - val_acc: 0.9172\n",
      "Epoch 19/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2380 - acc: 0.9099 - val_loss: 0.2273 - val_acc: 0.9163\n",
      "Epoch 20/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2356 - acc: 0.9122 - val_loss: 0.2278 - val_acc: 0.9197\n",
      "Epoch 21/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2329 - acc: 0.9138 - val_loss: 0.2207 - val_acc: 0.9193\n",
      "Epoch 22/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2302 - acc: 0.9148 - val_loss: 0.2261 - val_acc: 0.9212\n",
      "Epoch 23/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2298 - acc: 0.9135 - val_loss: 0.2172 - val_acc: 0.9222\n",
      "Epoch 24/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2239 - acc: 0.9156 - val_loss: 0.2162 - val_acc: 0.9207\n",
      "Epoch 25/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2262 - acc: 0.9161 - val_loss: 0.2232 - val_acc: 0.9198\n",
      "Epoch 26/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2242 - acc: 0.9156 - val_loss: 0.2170 - val_acc: 0.9217\n",
      "Epoch 27/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2198 - acc: 0.9171 - val_loss: 0.2230 - val_acc: 0.9195\n",
      "Epoch 28/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2181 - acc: 0.9172 - val_loss: 0.2111 - val_acc: 0.9267\n",
      "Epoch 29/30\n",
      "54000/54000 [==============================] - 55s 1ms/sample - loss: 0.2176 - acc: 0.9185 - val_loss: 0.2164 - val_acc: 0.9260\n",
      "Epoch 30/30\n",
      "54000/54000 [==============================] - 56s 1ms/sample - loss: 0.2152 - acc: 0.9202 - val_loss: 0.2224 - val_acc: 0.9235\n"
     ]
    }
   ],
   "source": [
    "train_model = model.fit(X_train, y_train,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=NO_EPOCHS,\n",
    "                  verbose=1,\n",
    "                  validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Précision de la prévision avec le nouveau modèle\n",
    "Réévaluons l'exactitude de la prédiction de test avec le nouveau modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2097082172483206\n",
      "Test accuracy: 0.9244\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "Avec un modèle séquentiel complexe avec plusieurs couches de convolution et 30 epochs pour l’entraînement, nous avons obtenu\n",
    "une précision de ~ 0,9121 pour la prédiction de test. Après avoir examiné la précision et les pertes de validation, nous avons\n",
    "compris que le modèle surapprenait.\n",
    "Nous avons  donc recyclé le modèle avec des couches de suppression  afin de réduire le surapprentissage.\n",
    "et dès lors on a pu confirmé l’amélioration du modèle puisque  nous avons obtenu avec le nouveau modèle une précision de ~ 0,9244 pour la prévision des tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
